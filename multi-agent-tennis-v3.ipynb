{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Continuous control\n",
    "## DDPG algorithm implementation\n",
    "\n",
    "---\n",
    "\n",
    "This self sufficient contains the whole code needed for an implementation of the DDPG reinforcement learning algorithm in the \"reacher environment\".\n",
    "\n",
    "We follow the implementation described in the paper \"Continuous control with reinforcement learning\": https://arxiv.org/abs/1509.02971"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unityagents import UnityEnvironment\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from collections import namedtuple, deque, defaultdict\n",
    "import random\n",
    "from matplotlib import pyplot as plt\n",
    "import pickle\n",
    "from IPython import display\n",
    "import pylab as pl\n",
    "import copy\n",
    "import time\n",
    "import os\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Depending on which version of the environment you want to run, select one line or the other in the cell below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed=916):   #911\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "seed_everything()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\t\n",
      "Unity brain name: TennisBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 8\n",
      "        Number of stacked Vector Observation: 3\n",
      "        Vector Action space type: continuous\n",
      "        Vector Action space size (per agent): 2\n",
      "        Vector Action descriptions: , \n"
     ]
    }
   ],
   "source": [
    "# Select this option to load version 1 (with a single agent) of the environment\n",
    "# env = UnityEnvironment(file_name='/Users/manuelsh/code/deep-reinforcement-learning/p2_continuous-control/Reacher.app')\n",
    "\n",
    "# select this option to load version 2 (with 20 agents) of the environment\n",
    "env = UnityEnvironment(file_name='/Users/manuelsh/code/deep-reinforcement-learning/p3_collab-compet/Tennis.app')\n",
    "\n",
    "# Get the default brain\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examine the State and Action Spaces\n",
    "\n",
    "Run the code cell below to print some information about the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of agents: 2\n",
      "Size of each action: 2\n",
      "There are 2 agents. Each observes a state with length: 24\n",
      "The state for the first agent looks like: [ 0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.         -6.65278625 -1.5\n",
      " -0.          0.          6.83172083  6.         -0.          0.        ]\n"
     ]
    }
   ],
   "source": [
    "# reset the environment\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "\n",
    "# number of agents\n",
    "num_agents = len(env_info.agents)\n",
    "print('Number of agents:', num_agents)\n",
    "\n",
    "# size of each action\n",
    "action_size = brain.vector_action_space_size\n",
    "print('Size of each action:', action_size)\n",
    "\n",
    "# examine the state space \n",
    "states = env_info.vector_observations\n",
    "state_size = states.shape[1]\n",
    "print('There are {} agents. Each observes a state with length: {}'.format(states.shape[0], state_size))\n",
    "print('The state for the first agent looks like:', states[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation of relevant classes\n",
    "In the following cells we implement the ReplayBuffer class (coming from the Udacity repo), Ornstein Uhlenbeck noise generation, soft-update function (which updates the parameters of the target function) and the classes for the actor (`DeterministicPolicy`) and the critic (`QFunction`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    \"\"\"Fixed-size buffer to store experience tuples.\"\"\"\n",
    "\n",
    "    def __init__(self, action_size, buffer_size, batch_size):\n",
    "        \"\"\"Initialize a ReplayBuffer object.\n",
    "\n",
    "        Params\n",
    "        ======\n",
    "            action_size (int): dimension of each action\n",
    "            buffer_size (int): maximum size of buffer\n",
    "            batch_size (int): size of each training batch\n",
    "            seed (int): random seed\n",
    "        \"\"\"\n",
    "        self.action_size = action_size\n",
    "        self.memory = deque(maxlen=buffer_size)  \n",
    "        self.batch_size = batch_size\n",
    "        self.experience = namedtuple(\"Experience\", field_names=[\"state\", \n",
    "                                                                \"action\", \n",
    "                                                                \"reward\", \n",
    "                                                                \"next_state\", \n",
    "                                                                \"done\",\n",
    "                                                                \"td_error\"])\n",
    "        self.td_errors = []\n",
    "    \n",
    "    def add(self, states, actions, rewards, next_states, dones, td_errors):\n",
    "        \"\"\"Add a new experience to memory.\"\"\"\n",
    "        for state, action, reward, next_state, done, td_error  in zip(states, actions, rewards, \n",
    "                                                                      next_states, dones, td_errors):\n",
    "            e = self.experience(state, action, reward, next_state, done, float(td_error))\n",
    "            self.memory.append(e)\n",
    "            self.td_errors.append(float(td_error))\n",
    "    \n",
    "    def sample(self):\n",
    "        \"\"\"Randomly sample a batch of experiences from memory.\"\"\"\n",
    "        \n",
    "        td_error_probs = self.get_td_errors_probs(self.td_errors)\n",
    "        elements = np.random.choice(range(len(self.memory)), size=BATCH_SIZE, \n",
    "                                    p=td_error_probs, replace=False)\n",
    "        experiences = [self.memory[element] for element in elements]\n",
    "        #from IPython.core.debugger import Tracer; Tracer()()\n",
    "        states = torch.from_numpy(np.vstack([e.state for e in experiences if e is not None])).float().to(device)\n",
    "        actions = torch.from_numpy(np.vstack([e.action for e in experiences if e is not None])).float().to(device)\n",
    "        rewards = torch.from_numpy(np.vstack([e.reward for e in experiences if e is not None])).float().to(device)\n",
    "        next_states = torch.from_numpy(np.vstack([e.next_state for e in experiences if e is not None])).float().to(device)\n",
    "        dones = torch.from_numpy(np.vstack([e.done for e in experiences if e is not None]).astype(np.uint8)).float().to(device)\n",
    "        td_errors = torch.from_numpy(np.vstack([e.td_error for e in experiences if e is not None])).float().to(device)\n",
    "  \n",
    "        return (states, actions, rewards, next_states, dones, td_errors)\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Return the current size of internal memory.\"\"\"\n",
    "        return len(self.memory)\n",
    "        \n",
    "    def get_td_errors_probs(self, td_errors):\n",
    "        td_error_probs_scaled = np.abs(td_errors)**PE_ALPHA\n",
    "        return td_error_probs_scaled / (np.abs(self.td_errors)**PE_ALPHA).sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OUNoise:\n",
    "    \"\"\"Ornstein-Uhlenbeck process.\"\"\"\n",
    "\n",
    "    def __init__(self, size, mu=0., theta=0.2, sigma=1, num_agents=num_agents, num_actions=action_size):\n",
    "        \"\"\"Initialize parameters and noise process.\"\"\"\n",
    "        self.mu = mu * np.ones(size)\n",
    "        self.theta = theta\n",
    "        self.sigma = sigma\n",
    "        self.num_agents = num_agents\n",
    "        self.num_actions = num_actions\n",
    "        self.state = copy.copy(self.mu)\n",
    "        self.size = size\n",
    "\n",
    "    def sample(self):\n",
    "        \"\"\"Update internal state and return it as a noise sample.\"\"\"\n",
    "        x = self.state\n",
    "        dx = self.theta * (self.mu - x) + self.sigma * np.random.standard_normal(self.size)\n",
    "        self.state = x + dx\n",
    "        return self.state.reshape(self.num_agents,self.num_actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def soft_update(local_model, target_model, tau):\n",
    "        \"\"\"Soft update model parameters.\n",
    "        θ_target = τ*θ_local + (1 - τ)*θ_target\n",
    "\n",
    "        Params\n",
    "        ======\n",
    "            local_model (PyTorch model): weights will be copied from\n",
    "            target_model (PyTorch model): weights will be copied to\n",
    "            tau (float): interpolation parameter \n",
    "        \"\"\"\n",
    "        for target_param, local_param in zip(target_model.parameters(), local_model.parameters()):\n",
    "            target_param.data.copy_(tau*local_param.data + (1.0-tau)*target_param.data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QFunction(nn.Module):\n",
    "    def __init__(self, state_dim, actions_dim, hidden_units=(400, 300,200), activation=F.relu):\n",
    "        super(QFunction, self).__init__()\n",
    "        dims = (state_dim + actions_dim, ) + hidden_units + (1, )\n",
    "        self.layers = nn.ModuleList(\n",
    "            [nn.Linear(dim_in, dim_out) for dim_in, dim_out in zip(dims[:-1], dims[1:])])\n",
    "        self.activation = activation      \n",
    "        \n",
    "    def forward(self, state, action):\n",
    "        x = torch.cat([state, action], dim=1)\n",
    "        for layer in self.layers[:-1]:\n",
    "            x = self.activation(layer(x))\n",
    "        return self.layers[-1](x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeterministicPolicy(nn.Module):\n",
    "    def __init__(self, state_dim, actions_dim, hidden_units=(400, 300,200) , activation=F.relu):\n",
    "        super(DeterministicPolicy, self).__init__()\n",
    "        dims = (state_dim,) + hidden_units +(actions_dim,)\n",
    "        self.layers = nn.ModuleList(\n",
    "            [nn.Linear(dim_in, dim_out) for dim_in, dim_out in zip(dims[:-1], dims[1:])])\n",
    "        self.activation = activation      \n",
    "        \n",
    "    def forward(self, state):\n",
    "        x = state\n",
    "        for layer in self.layers[:-1]:\n",
    "            x = self.activation(layer(x))\n",
    "        return F.tanh( self.layers[-1](x) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_an_action(states, noise):\n",
    "        # perform an action\n",
    "        states_tensor = torch.FloatTensor(states).to(device)\n",
    "        actions = agent(states_tensor).detach().cpu().numpy() + noise.sample()\n",
    "        env_info = env.step(actions)[brain_name]\n",
    "        next_states = env_info.vector_observations \n",
    "        rewards = env_info.rewards                         \n",
    "        dones = env_info.local_done\n",
    "        return states, actions, rewards, next_states, dones      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def watch_env(t_max=600):\n",
    "    time.sleep(0.1)\n",
    "    env_info = env.reset(train_mode=False)[brain_name]      # reset the environment    \n",
    "    states = env_info.vector_observations                  # get the current state (for each agent)\n",
    "    scores = np.zeros(num_agents)                          # initialize the score (for each agent)\n",
    "    noise = OUNoise(size=num_agents*action_size, theta=OU_THETA, sigma=OU_SIGMA_END)\n",
    "    noise_sample = noise.sample()\n",
    "    for i in range(t_max):\n",
    "        states, actions, rewards, next_states, dones = perform_an_action(states, noise)\n",
    "        scores += rewards                         # update the score (for each agent)\n",
    "        states = next_states                               # roll over states to next time step\n",
    "        time.sleep(0.01)\n",
    "        if np.any(dones):                                  # exit loop if episode finished\n",
    "            break\n",
    "    print('Total score (averaged over agents) this episode: {}'.format(np.mean(scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters, instantiation and training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main parameters:\n",
    "\n",
    "BUFFER_SIZE = int(1e6)\n",
    "BATCH_SIZE = 128      \n",
    "LR_CRITIC = 1E-3\n",
    "LR_AGENT = 1E-3\n",
    "\n",
    "GAMMA = 0.99\n",
    "TAU = 0.008\n",
    "\n",
    "UPDATE_STEPS = 1\n",
    "NUM_UPDATES = 1\n",
    "\n",
    "OU_SIGMA_START = 3\n",
    "OU_SIGMA_END = 0\n",
    "OU_THETA = 0.5\n",
    "EPIS_SIGMA_END = 300 # number of episodes to set sigma to zero\n",
    "\n",
    "PE_ALPHA = 0.7\n",
    "PE_BETA_START = 0.5 # prioritised experience replay beta start\n",
    "PE_BETA_END = 1 # prioritised experience replay beta end\n",
    "EPIS_PE_BETA_END = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiation\n",
    "critic = QFunction(state_size, action_size).to(device)\n",
    "agent = DeterministicPolicy(state_size, action_size).to(device)\n",
    "\n",
    "critic_target = QFunction(state_size, action_size).to(device)\n",
    "agent_target = DeterministicPolicy(state_size, action_size).to(device)\n",
    "\n",
    "critic_target.load_state_dict(critic.state_dict())\n",
    "agent_target.load_state_dict(agent.state_dict())\n",
    "\n",
    "replay_buffer = ReplayBuffer(action_size=action_size, buffer_size=BUFFER_SIZE, batch_size=BATCH_SIZE)\n",
    "\n",
    "critic_loss_function = torch.nn.MSELoss()\n",
    "critic_optimizer = torch.optim.Adam(critic.parameters(), lr=LR_CRITIC)\n",
    "agent_optimizer = torch.optim.Adam(agent.parameters(), lr=LR_AGENT)\n",
    "\n",
    "# Initialization of neede dvariables\n",
    "total_scores = []\n",
    "total_scores_queue = deque(maxlen=100)\n",
    "running_averages = []\n",
    "monitoring = defaultdict(list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-7b2084d8a6d9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mpl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m12\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepisode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_episodes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0menv_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_mode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbrain_name\u001b[0m\u001b[0;34m]\u001b[0m      \u001b[0;31m# reset the environment\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0mstates\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv_info\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvector_observations\u001b[0m                  \u001b[0;31m# get the current state (for each agent)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_agents\u001b[0m\u001b[0;34m)\u001b[0m                          \u001b[0;31m# initialize the score (for each agent)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda/envs/drlnd/lib/python3.6/site-packages/unityagents/environment.py\u001b[0m in \u001b[0;36mreset\u001b[0;34m(self, train_mode, config, lesson)\u001b[0m\n\u001b[1;32m    259\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_loaded\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    260\u001b[0m             outputs = self.communicator.exchange(\n\u001b[0;32m--> 261\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_generate_reset_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_mode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    262\u001b[0m             )\n\u001b[1;32m    263\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0moutputs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda/envs/drlnd/lib/python3.6/site-packages/unityagents/rpc_communicator.py\u001b[0m in \u001b[0;36mexchange\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m     76\u001b[0m         \u001b[0mmessage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munity_input\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCopyFrom\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munity_to_external\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparent_conn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munity_to_external\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparent_conn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     79\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mheader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m200\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda/envs/drlnd/lib/python3.6/multiprocessing/connection.py\u001b[0m in \u001b[0;36mrecv\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    248\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_closed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_readable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 250\u001b[0;31m         \u001b[0mbuf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_recv_bytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    251\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_ForkingPickler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetbuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda/envs/drlnd/lib/python3.6/multiprocessing/connection.py\u001b[0m in \u001b[0;36m_recv_bytes\u001b[0;34m(self, maxsize)\u001b[0m\n\u001b[1;32m    405\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    406\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_recv_bytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 407\u001b[0;31m         \u001b[0mbuf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_recv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    408\u001b[0m         \u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstruct\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munpack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"!i\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetvalue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    409\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmaxsize\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0msize\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mmaxsize\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda/envs/drlnd/lib/python3.6/multiprocessing/connection.py\u001b[0m in \u001b[0;36m_recv\u001b[0;34m(self, size, read)\u001b[0m\n\u001b[1;32m    377\u001b[0m         \u001b[0mremaining\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    378\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0mremaining\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 379\u001b[0;31m             \u001b[0mchunk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mremaining\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    380\u001b[0m             \u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    381\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mn\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 864x360 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 864x360 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "num_episodes = 4000\n",
    "step_counter = 0\n",
    "pl.figure(1, figsize=(12,5))\n",
    "pl.figure(2, figsize=(12,5))\n",
    "for episode in range(num_episodes):\n",
    "    env_info = env.reset(train_mode=True)[brain_name]      # reset the environment    \n",
    "    states = env_info.vector_observations                  # get the current state (for each agent)\n",
    "    scores = np.zeros(num_agents)                          # initialize the score (for each agent)\n",
    "    ou_sigma= OU_SIGMA_START - (OU_SIGMA_START - OU_SIGMA_END) * min(1., episode/EPIS_SIGMA_END)\n",
    "    noise = OUNoise(num_agents*action_size, sigma=ou_sigma, theta=OU_THETA)\n",
    "    beta = PE_BETA_START - (PE_BETA_START - PE_BETA_END) * min(1., episode/EPIS_PE_BETA_END)\n",
    "    while True:\n",
    "        \n",
    "        states, actions, rewards, next_states, dones = perform_an_action(states, noise)\n",
    "        next_states_tensor = torch.FloatTensor(next_states).to(device)\n",
    "        states_tensor = torch.FloatTensor(states).to(device)\n",
    "        actions_tensor = torch.FloatTensor(actions).to(device)\n",
    "        rewards_tensor = torch.FloatTensor(rewards).to(device)\n",
    "        dones_tensor =  torch.FloatTensor(dones).to(device)\n",
    "        critic_target_value =  critic_target(next_states_tensor,\n",
    "                                             agent_target(next_states_tensor)).squeeze() * (1-dones_tensor) \n",
    "        \n",
    "        td_errors = rewards_tensor + GAMMA *critic_target_value - critic(states_tensor, actions_tensor).squeeze()\n",
    "        replay_buffer.add(states=states, \n",
    "                          actions=actions, \n",
    "                          rewards=rewards, \n",
    "                          next_states=next_states, \n",
    "                          dones=dones,\n",
    "                          td_errors=td_errors)\n",
    "        scores += rewards                         # update the score (for each agent)\n",
    "        states = next_states                    # roll over states to next time step \n",
    "        step_counter+=1  \n",
    "        if (len(replay_buffer.memory)>BATCH_SIZE) & (step_counter%UPDATE_STEPS==0) :\n",
    "            for i in range(NUM_UPDATES):\n",
    "                # Train the critic\n",
    "                states_r, actions_r, rewards_r, next_states_r, dones_r, td_errors_r = replay_buffer.sample()\n",
    "                weight = 1/ ( replay_buffer.get_td_errors_probs(td_errors_r) * len(replay_buffer))**beta\n",
    "                critic_target_result = critic_target(next_states_r, agent_target(next_states_r)) * (1-dones_r)\n",
    "                \n",
    "                y = rewards_r + \\\n",
    "                    GAMMA * critic_target_result \n",
    "            \n",
    "                y = weight.to(device) * y.detach()\n",
    "                \n",
    "                critic_result = critic(states_r, actions_r)\n",
    "                critic_loss_value = critic_loss_function(critic_result, y)\n",
    "                critic_optimizer.zero_grad()\n",
    "                critic_loss_value.backward()\n",
    "                critic_optimizer.step()\n",
    "\n",
    "                # Train the agent\n",
    "                agent_loss_value = -critic(states_r, agent(states_r)).mean()\n",
    "                agent_optimizer.zero_grad()\n",
    "                agent_loss_value.backward()\n",
    "                agent_optimizer.step()\n",
    "\n",
    "                # Update parameters\n",
    "                soft_update(agent, agent_target, TAU)\n",
    "                soft_update(critic, critic_target, TAU)\n",
    "                \n",
    "                monitoring['critic_target_result'].append(float(critic_target_result.mean()))\n",
    "                monitoring['y'].append(float(y.mean()))\n",
    "                monitoring['critic_result'].append(float(critic_result.mean()))\n",
    "               \n",
    "        if np.any(dones):                                  # exit loop if episode finished\n",
    "            break\n",
    "            \n",
    "    #print('Total score (averaged over agents) this episode: {}'.format(np.mean(scores)))\n",
    "    total_scores.append(np.max(scores))\n",
    "    total_scores_queue.append(np.max(scores))\n",
    "    running_average = np.mean(total_scores_queue)\n",
    "    running_averages.append(running_average)\n",
    "    pickle.dump(total_scores, open('total_scores.pickle','wb'))\n",
    "    pickle.dump(running_average, open('running_average.pickle','wb'))\n",
    "    pickle.dump(monitoring, open('monitoring.pickle','wb'))\n",
    "    \n",
    "    # Plot\n",
    "    display.clear_output(wait=True)\n",
    "    \n",
    "    pl.figure(1)\n",
    "    pl.plot(total_scores, label='Episode score')\n",
    "    pl.plot(running_averages, label='Running average score, 100 episodes', linestyle='--')\n",
    "    display.display(pl.gcf())\n",
    "    \n",
    "    pl.figure(2)\n",
    "    for key in monitoring.keys():\n",
    "        pl.plot(monitoring[key], label=key)\n",
    "    display.display(pl.gcf())\n",
    "    \n",
    "    print(f'Running average episode {len(total_scores)}: {running_average}')\n",
    "    if running_average>=0.5:\n",
    "        print(f'Problem solved at episode {len(total_scores)}')\n",
    "        break\n",
    "        \n",
    "    if len(total_scores)%25==0:\n",
    "        watch_env()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saves parameters of model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(agent.state_dict(), 'models-parameters/agent_reacher.torch')\n",
    "torch.save(critic.state_dict(), 'models-parameters/critic_reacher.torch')\n",
    "torch.save(agent_target.state_dict(), 'models-parameters/agent_target_reacher.torch')\n",
    "torch.save(critic_target.state_dict(), 'models-parameters/critic_target_reacher.torch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time.sleep(0.1)\n",
    "env_info = env.reset(train_mode=False)[brain_name]      # reset the environment    \n",
    "states = env_info.vector_observations                  # get the current state (for each agent)\n",
    "scores = np.zeros(num_agents)                          # initialize the score (for each agent)\n",
    "noise.reset()\n",
    "noise_sample = noise.sample()\n",
    "states = np.append(states, noise_sample, 1)\n",
    "while True:\n",
    "    states, actions, rewards, next_states, dones, next_noise = perform_an_action(states, noise_sample)\n",
    "    scores += rewards                         # update the score (for each agent)\n",
    "    states = next_states                               # roll over states to next time step\n",
    "    time.sleep(0.01)\n",
    "print('Total score (averaged over agents) this episode: {}'.format(np.mean(scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in monitoring.keys():\n",
    "    pl.plot(monitoring[key], label=key)\n",
    "pl.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise.mu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise = OUNoise(num_agents*action_size, sigma=ou_sigma, theta=0.2)\n",
    "pl.plot([noise.sample()[0][0] for i in range(100)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "drlnd",
   "language": "python",
   "name": "drlnd"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
